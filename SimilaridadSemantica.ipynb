{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Importar librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cargar la base de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e955162415b74f1aae514a5b68e4b3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc75cf27088458a9fa955fe9b63bed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/19.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c308184a2f4a4619af32eb23d9001b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/9.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7269974d20845f1a3c50b77a955faac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b295389ca9a84617921c732fccf1fec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f23f4daea24dbe9eacba2e4145e26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/74.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5fe39d887248eab3660b609feff112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/52.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35307f310dc4c07963f396e1a1c7843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5749 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2653f332705e4bdf9eeb2feb932e920d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1379 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85c91c01b534ea986c7687220604da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5749, 3) (1379, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A girl is styling her hair.</td>\n",
       "      <td>A girl is brushing her hair.</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A group of men play soccer on the beach.</td>\n",
       "      <td>A group of boys are playing soccer on the beach.</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One woman is measuring another woman's ankle.</td>\n",
       "      <td>A woman measures another woman's ankle.</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A man is cutting up a cucumber.</td>\n",
       "      <td>A man is slicing a cucumber.</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is playing a harp.</td>\n",
       "      <td>A man is playing a keyboard.</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence1  \\\n",
       "0                    A girl is styling her hair.   \n",
       "1       A group of men play soccer on the beach.   \n",
       "2  One woman is measuring another woman's ankle.   \n",
       "3                A man is cutting up a cucumber.   \n",
       "4                       A man is playing a harp.   \n",
       "\n",
       "                                          sentence2  similarity_score  \n",
       "0                      A girl is brushing her hair.               2.5  \n",
       "1  A group of boys are playing soccer on the beach.               3.6  \n",
       "2           A woman measures another woman's ankle.               5.0  \n",
       "3                      A man is slicing a cucumber.               4.2  \n",
       "4                      A man is playing a keyboard.               1.5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the English STSB dataset\n",
    "stsb_dataset = load_dataset('stsb_multi_mt', 'en')\n",
    "stsb_train = pd.DataFrame(stsb_dataset['train'])\n",
    "stsb_test = pd.DataFrame(stsb_dataset['test'])\n",
    "\n",
    "# Check loaded data\n",
    "print(stsb_train.shape, stsb_test.shape)\n",
    "stsb_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Crear Funciones Útiles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera función tiene como objetivo preprocesar textos mediante la lematización, convertir a minúsculas y eliminar números y palabras de parada. \n",
    "La segunda función toma dos columnas de incrustaciones de texto y devuelve la similitud del coseno entre las dos columnas a nivel de filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def text_processing(sentence):\n",
    "    \"\"\"\n",
    "    Lemmatize, lowercase, remove numbers and stop words\n",
    "    \n",
    "    Args:\n",
    "      sentence: The sentence we want to process.\n",
    "    \n",
    "    Returns:\n",
    "      A list of processed words\n",
    "    \"\"\"\n",
    "    sentence = [token.lemma_.lower()\n",
    "                for token in nlp(sentence) \n",
    "                if token.is_alpha and not token.is_stop]\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "def cos_sim(sentence1_emb, sentence2_emb):\n",
    "    \"\"\"\n",
    "    Cosine similarity between two columns of sentence embeddings\n",
    "    \n",
    "    Args:\n",
    "      sentence1_emb: sentence1 embedding column\n",
    "      sentence2_emb: sentence2 embedding column\n",
    "    \n",
    "    Returns:\n",
    "      The row-wise cosine similarity between the two columns.\n",
    "      For instance is sentence1_emb=[a,b,c] and sentence2_emb=[x,y,z]\n",
    "      Then the result is [cosine_similarity(a,x), cosine_similarity(b,y), cosine_similarity(c,z)]\n",
    "    \"\"\"\n",
    "    cos_sim = cosine_similarity(sentence1_emb, sentence2_emb)\n",
    "    return np.diag(cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Medir similaridad semántica con algoritmos clasicos sin contexto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "\n",
    "def jaccard_sim(row):\n",
    "    # Text Processing\n",
    "    sentence1 = text_processing(row['sentence1'])\n",
    "    sentence2 = text_processing(row['sentence2'])\n",
    "    \n",
    "    # Jaccard similarity\n",
    "    return textdistance.jaccard.normalized_similarity(sentence1, sentence2)\n",
    "\n",
    "\n",
    "# Jaccard Similarity\n",
    "stsb_test['Jaccard_score'] = stsb_test.progress_apply(jaccard_sim, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "model = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Train the model\n",
    "X_train = pd.concat([stsb_train['sentence1'], stsb_train['sentence2']]).unique()\n",
    "model.fit(X_train)\n",
    "\n",
    "# Generate Embeddings on Test\n",
    "sentence1_emb = model.transform(stsb_test['sentence1'])\n",
    "sentence2_emb = model.transform(stsb_test['sentence2'])\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['TFIDF_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Movers Distance (WMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "def word_movers_distance(row):\n",
    "    # Text Processing\n",
    "    sentence1 = text_processing(row['sentence1'])\n",
    "    sentence2 = text_processing(row['sentence2'])\n",
    "    \n",
    "    # Negative Word Movers Distance\n",
    "    return -model.wmdistance(sentence1, sentence2)\n",
    "\n",
    "\n",
    "# Negative Word Movers Distance\n",
    "stsb_test['NegWMD_score'] = stsb_test.progress_apply(word_movers_distance, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Medir Similaridad Semántica con algoritmos modernos con contexto**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Sentence Encoder (USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load the pre-trained model\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    # Control GPU memory usage\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n",
    "model = hub.load(module_url)\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model(stsb_test['sentence1']).numpy()\n",
    "sentence2_emb = model(stsb_test['sentence2']).numpy()\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['USE_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = CrossEncoder('cross-encoder/stsb-roberta-base')\n",
    "\n",
    "sentence_pairs = []\n",
    "for sentence1, sentence2 in zip(stsb_test['sentence1'], stsb_test['sentence2']):\n",
    "    sentence_pairs.append([sentence1, sentence2])\n",
    "    \n",
    "stsb_test['SBERT CrossEncoder_score'] = model.predict(sentence_pairs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('stsb-mpnet-base-v2')\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model.encode(stsb_test['sentence1'], show_progress_bar=True)\n",
    "sentence2_emb = model.encode(stsb_test['sentence2'], show_progress_bar=True)\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['SBERT BiEncoder_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimCSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Supervised ##########\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('princeton-nlp/sup-simcse-roberta-large')\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model.encode(stsb_test['sentence1'], show_progress_bar=True)\n",
    "sentence2_emb = model.encode(stsb_test['sentence2'], show_progress_bar=True)\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['SimCSE Supervised_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)\n",
    "\n",
    "\n",
    "########## Un-Supervised ##########\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('princeton-nlp/unsup-simcse-roberta-large')\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = model.encode(stsb_test['sentence1'], show_progress_bar=True)\n",
    "sentence2_emb = model.encode(stsb_test['sentence2'], show_progress_bar=True)\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['SimCSE Unsupervised_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import openai\n",
    "import os\n",
    "import pickle\n",
    "openai.api_key = 'update_your_openai_API_key_here'\n",
    "\n",
    "if os.path.exists('../data/nlp/davinci_emb.pkl'):\n",
    "    print('Loading Davinci Embeddings')\n",
    "    with open('../data/nlp/davinci_emb.pkl', 'rb') as f:\n",
    "        davinci_emb = pickle.load(f)\n",
    "else:\n",
    "    print('Querying Davinci Embeddings')\n",
    "    davinci_emb = {}\n",
    "    engine='text-similarity-davinci-001'\n",
    "\n",
    "    unique_sentences = list(set(stsb_test['sentence1'].values.tolist() + stsb_test['sentence2'].values.tolist()))\n",
    "    for sentence in tqdm(unique_sentences):\n",
    "        if sentence not in davinci_emb.keys():\n",
    "            davinci_emb[sentence] = openai.Embedding.create(input = [sentence], \n",
    "                                                            engine=engine)['data'][0]['embedding']\n",
    "    # Save embeddings to file      \n",
    "    with open('../data/nlp/davinci_emb.pkl', 'wb') as f:\n",
    "        pickle.dump(davinci_emb, f)\n",
    "\n",
    "# Generate Embeddings\n",
    "sentence1_emb = [davinci_emb[sentence] for sentence in stsb_test['sentence1']]\n",
    "sentence2_emb = [davinci_emb[sentence] for sentence in stsb_test['sentence2']]\n",
    "\n",
    "# Cosine Similarity\n",
    "stsb_test['OpenAI Davinci_cosine_score'] = cos_sim(sentence1_emb, sentence2_emb)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cols = [col for col in stsb_test.columns if '_score' in col]\n",
    "\n",
    "# Spearman Rank Correlation\n",
    "spearman_rank_corr = stsb_test[score_cols].corr(method='spearman').iloc[1:, 0:1]*100\n",
    "spearman_rank_corr.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "nrows = 4\n",
    "ncols = 3\n",
    "plot_array = np.arange(0, nrows*ncols).reshape(nrows, ncols)\n",
    "\n",
    "subplot_titles = [f'{row.Index.split(\"_\")[0]}: {row.similarity_score:.2f}' for row in spearman_rank_corr.itertuples()]\n",
    "fig = make_subplots(rows=nrows, cols=ncols, subplot_titles=subplot_titles)\n",
    "\n",
    "for index, score in enumerate(spearman_rank_corr.index):\n",
    "    row, col = np.argwhere(plot_array == index)[0]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=stsb_test[score_cols[0]], \n",
    "            y=stsb_test[score],\n",
    "            mode='markers',\n",
    "        ),\n",
    "        row=row+1, col=col+1\n",
    "    )\n",
    "\n",
    "\n",
    "fig.update_layout(height=700, width=1000, title_text='Spearman Rank Correlation (ρ × 100)', showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Referencias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leo, M. S. (2022, abril 25). Semantic textual similarity. Towards Data Science. https://towardsdatascience.com/semantic-textual-similarity-83b3ca4a840e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
